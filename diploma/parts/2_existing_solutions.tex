\section{Existing methods}

\subsection{Machine learning methods} \label{dl_methods}
There are a variety of LLM (Large language model) architectures that were applied to the task of language modeling. One significant example is \texttt{LSTM} (Long short-term memory) model, that was introduced by \textcite{lstm_1997}. \texttt{LSTM} is a variation of \texttt{RNN} (Recurrent neural network), and it was widely applied to language modeling, including morphology modeling. Another more recent significant example is the transformer architecture presented by \textcite{transformer_2017}, off which two years later \texttt{BERT} model was based \parencite{devlin_2019}. 

One of the biggest downsides of ML methods is that its quality depends on training data quantity, which makes it challenging to apply to low-resourse languages such as Shughni. However, with introduction of LLMs this problem was shown to be solvable, for example, as shown by developers of \texttt{UDify} model \parencite{kondratyuk_straka_model_2019}. In their work authors show, that a \texttt{BERT} model pretrained on a large corpus of 104 languages can be fine-tuned on very little amounts of other languages' data and still show decent results. For an example, they report that for Belarusian, \texttt{UDify} model achieved $UFeats=89.36\%$ (accuracy of tagging Universal Features) after training on only 261 sentences from `Belarusian HSE' Universal Dependencies treebank \parencite[Table 7]{kondratyuk_straka_model_2019}.

However, working with LLM models is a highly resource-demanding task. The authors of \texttt{UDify} state, that the fine-tuning of their model for a new language would require at least 16 Gigabytes of RAM and at least 12 Gigabytes of GPU video memory, and the training process would take at least 20 days depending on the GPU model. While a deep learning approach would be interesting to explore, such computational resources are not available for this project. The neural approach is not the main target of this work and is implemented. 

\subsection{Rule-based methods}
\subsubsection{Finite-state transducers}
The Rule-based approach historically is usually applied with the help of Finite-state transducers (FST), which is a variation of Finite-state machine, a mathematical abstract computational model. Following the terminology of Turing machines \parencite{Turing_1937}, a FST has two tapes: the input tape and the output tape. At any point it can read a next symbol from the input tape and then write a symbol to the output tape. Once a symbol was read from the input tape, it can not be read again, as the input tape shifts one symbol forward. 

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.5]{\rootdir/img/transducer1.png}
    \caption{An example of FST with a single initial state (most left node) and a single final state (most right node) for a language where only three words exist: \textit{works}, \textit{working} and \textit{worked}. The word \textit{worker}, for example will not be recognized as a valid word by this FST, since there is no 'd' transition at state \textit{worke}. The only way from \textit{worke} state is via 'd' transition, which corresponds to the \textit{worked} word. \parencite{beesley_fst_2002}}
    \label{fig:fst1}
\end{figure}

The inner structure of FST can be illustrated as a directed graph with a set of all \textit{states} (represented by graph's nodes), a set of \textit{transitions} (represented by graph's edges), a set of \textit{initial states} (a subset of all the states, these are states where FST can start reading from the input tape) and a set of \textit{final states} (a subset of all the states, these are the states where FST can stop reading from the input tape). A simplified FST is shown on Figure \ref{fig:fst1}. The letters above the graph's edges denote \textit{transition} rules, for an example \textit{transition} `w' means `read \textit{w} from the input tape THEN write \textit{w} to the output tape'.

While working, FST will only make transitions that are possible from the current state. If there are no valid transitions then FST fails to process the input, and the input is considered to be impossible in the current language model. Of course, the ideal FST model of a language has valid paths for all the grammatical wordforms and does not have any valid paths for any ungrammatical wordforms. The measure of the amount of language's grammatical wordforms that successfully pass through the FST from an \textit{initial state} to a \textit{final state} will be called \textit{Coverage} from now and on. The measure of the amount of language's ungrammatical wordforms that successfully pass through the FST from an \textit{initial state} to a \textit{final state} will be called \textit{Overgeneration} from now and on.

The model from the Figure \ref{fig:fst1} works effectively as a wordform paradigm dictionary, echoing back input wordforms that are grammatical and failing to output the whole ungrammatical wordforms. Now we can slightly adjust the transition rules in put example to make a morphological analysis tool that can bee seen on the Figure \ref{fig:fst1_1}. The notation of the \textit{transition} `w:w' is an alias for the notation `w' from the Figure \ref{fig:fst1}. If the spot on the right side of the semicolon (`:') sign is left empty, it means `write nothing to the output tape'. An important note to remember is that FST can output only one symbol to the output while making a single transition. In this example \textit{`<inf>'}, \textit{`<pst>'} and \textit{`<prs><2sg>'} are treated as `multichar' symbols, meaning they are treated as three individual symbols by a FST, it will be covered in more detail in the Section \ref{methods_section}.

\subsubsection{FST formalisms}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.5]{\rootdir/img/transducer1_1.png}
    \caption{A modified version of Figure \ref{fig:fst1} which takes as input \textit{works}, \textit{working}, \textit{worked} and outputs \textit{work<prs><2sg>}, \textit{work<inf>}, \textit{work<pst>} respectively.}
    \label{fig:fst1_1}
\end{figure}

\subsection{Existing morphology models for Shughni}
At this time only one morphological parser exists for Shughni. It was developed by \textcite{melenchenko_2021_parser} and was later included in `Digital Resources for the Shughni Language' project \parencite{makarov_digital_2022}. It is a rule-based parser implemented in Python which shows good coverage and accuracy results 