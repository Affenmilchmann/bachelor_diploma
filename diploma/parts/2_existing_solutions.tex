\section{Existing methods and solutions}

\subsection{Machine learning methods} \label{dl_methods}
There are a variety of LLM (Large language model) architectures that were applied to the task of language modeling. One significant example is \texttt{LSTM} (Long short-term memory) model, that was introduced by \textcite{lstm_1997}. \texttt{LSTM} is a variation of \texttt{RNN} (Recurrent neural network), and it was widely applied to language modeling, including morphology modeling. Another more recent significant example is the transformer architecture presented by \textcite{transformer_2017}, off which two years later \texttt{BERT} model was based \parencite{devlin_2019}. 

One of the biggest downsides of ML methods is that its quality depends on training data quantity, which makes it challenging to apply to low-resourse languages such as Shughni. However, with introduction of LLMs this problem was shown to be solvable, for example, as shown by developers of \texttt{UDify} model \parencite{kondratyuk_straka_model_2019}. In their work authors show, that a \texttt{BERT} model pretrained on a large corpus of 104 languages can be fine-tuned on very little amounts of other languages' data and still show decent results. For an example, they report that for Belarusian, \texttt{UDify} model achieved $UFeats=89.36\%$ (accuracy of tagging Universal Features) after training on only 261 sentences from `Belarusian HSE' Universal Dependencies treebank \parencite[Table 7]{kondratyuk_straka_model_2019}.

However, working with LLM models is a highly resource-demanding task. The authors of \texttt{UDify} state, that the fine-tuning of their model for a new language would require at least 16 Gigabytes of RAM and at least 12 Gigabytes of GPU video memory, and the training process would take at least 20 days depending on the GPU model. While a deep learning approach would be interesting to explore, such computational resources are not available for this project. The neural approach is not the main target of this work and is implemented. 

\subsection{Rule-based methods}


\subsection{Existing morphology models for Shughni}
At this time only one morphological parser exists for Shughni. It was developed by \textcite{melenchenko_2021_parser} and was later included in `Digital Resources for the Shughni Language' project \parencite{makarov_digital_2022}. It is a rule-based parser implemented in Python