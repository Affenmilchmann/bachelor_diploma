\subsection{Metrics} \label{metrics_section}
\subsubsection*{Coverage}
\textit{Coverage} metric is defined as relation of the amount tokens that were given any glossed output by FST ($N_{recognized}$) to the total amount of tokens ($N_{total}$) given to the FST:

\[Coverage = \frac{N_{recognized}}{N_{total}}\]

$N_{recognized}$ does not take into account if the given glosses are correct. It only shows the fraction of words that exist in the FST model's paradigm. 

The evaluation is done via a \texttt{Python} script \texttt{scripts/coverage/eval.py}. It reads Shughni plain text from \texttt{stdin}, which must be cleared of all punctuation. Then the script tokenizes it and feeds every token to the FST. Input texts come both in Cyrillic and Latin scripts, so the script detects writing system first and calls corresponding FST variation (\texttt{sgh\_analyze\_stem\_word\_cyr.hfst} or \texttt{sgh\_analyze\_stem\_word\_lat.hfst}). The script also calculates 5 most frequent unrecognized words and 5 most frequent unrecognized morphemes (it considers any hyphen-separated word fragments as morphemes).

\textit{Coverage} evaluation is integrated in the \texttt{Makefile}, an example of its output is shown in Code block \ref{code:10_1}.

\begin{code_frame}[float,floatplacement=!htbp]
    \begin{footnotesize}\codespacing
    \begin{verbatim}
$ make coverage 
cat scripts/coverage/corpus/* | scripts/coverage/eval.py
Coverage corpus
╭──────────┬─────────┬───────────────╮
│ metric   │   value │ absolute      │
├──────────┼─────────┼───────────────┤
│ coverage │  82.65% │ 154685/187149 │
╰──────────┴─────────┴───────────────╯
Top 5 unrecognized (morphs):
[('на', 914), ('инд', 548), ('ат', 520), ('та', 456), ('ирд', 429)]
Top 5 unrecognized (words):
[('су̊д', 619), ('ғал', 601), ('дāк', 310), ('čīdōw', 304), ('ку', 276)]
    \end{verbatim}
    \end{footnotesize}
    \tcblower
    \captionof{Code}{ An example of coverage script work. \textbf{Not actual results!} \todo{maybe actual results, lets see}}
    \label{code:10_1}
\end{code_frame}

\subsubsection*{Accuracy}
\textit{Accuracy} metric is defined as relation of the amount of tokens, to which a FST has given a correct output ($N_{correct}$) to the amount of tokens, which were given any glossed output by the FST ($N_{recognized}$):

\[Accuracy = \frac{N_{correct}}{N_{recognized}}\]

$N_{recognized}$ in the accuracy metric is the same as $N_{recognized}$ in the coverage metric. 

