\section{Literature review}
\subsection{Shughni}
\par There are two main dictionaries of the Shughni language: one by \textcite{zarubin_dict_1960} and one by \textcite{karamshoev_dict_1988}, both are written using Cyrillic script and include Russian translations. Some early dictionaries are `Brief grammar and dictionary of Shughni' \parencite{tumanovich_gram_1906}, that is also using Cyrillic and translates to Russian, and `Shughni dictionary by D. L. Ivanov' \parencite{salemann_dict_1895}, that translates to Russian but uses Arabic script alongside Cyrillic transcriptions for Shughhi word-forms.
\par Several Shughni grammar descriptions were written throughout the years, starting from basic grammar description done by D. L. Ivanov \parencite[274-281]{salemann_dict_1895}. An important mention is a work by \textcite{karamshoev_dialect_1963}, which was the most detailed Shughni grammar description of its time. Latest significant works were `Shughni language' \parencite[225-242]{edelman_languages_1999}, 'Comparative Grammar of Eastern Iranian Languages' \parencite{edelman_gram_2009} and `A grammar of the Shughhi language' by \textcite{parker_shughni_2023}, which is the biggest existing grammar, the most detailed and the most recent one.
\par A significant contribution to the Shughni NLP field is `Digital Resources for the Shughni language' \parencite{makarov_digital_2022}. The authors, among other tools and resources, developed a rule-based morphological analysis tool for the Shughni language \textcite{melchenko_2021_parser}. The parser proposed in this work, while also being rule-based, differs in its implementation through the use of Helsinki Finite-State Technology (HFST).

\subsection{Morphology modeling}
\subsubsection{Neural approach}
\par One of the most recent and widely adopted approaches to morphology modeling involves the use of the Transformer-based deep learning models. This approach usually requires large amounts of training data in form of manually tagged word-forms, which is not available for Shughni. There are texts available to me, that were manually tagged as part of `Digital Resources for the Shughni Language' project \parencite{makarov_digital_2022}, which consist of 3453 tokens in total. While this amount of training data is small, it is worth mentioning that it is possible to train a Transformer-based model with such small datasets, as shown by the developers of \texttt{UDify} model \parencite{kondratyuk_straka_model_2019}. Their approach includes fine-tuning a pre-trained multilingual BERT model \parencite{devlin-etal-2019-bert}, authors conclude, that multilingual learning is most beneficial for low-resource languages, even ones that do not possess a training set. 
\par The neural approach is not the main target of this work and is not planned to be implemented. Working with BERT models is a highly resource-demanding task. The authors of \texttt{UDify} state, that the training of their model for a new language would require at least 16 Gigabytes of RAM and at least 12 Gigabytes of GPU video memory, and the training process would take at least 20 days depending on the video card model. While a deep learning approach would be interesting to explore, such computational resources are not available for this project.
\subsubsection{Rule-based approach}
\par Finite-state technology (FST) is a finite-state machine with two tapes, one for input strings and one for output strings. The machine maps the alphabet of the first string to the alphabet of the second string, this concept was first proposed by \textcite{mealy_method_1955} and \textcite{moore_gedanken_1955}. Eventually linguists noticed this technology and started applying it to model natural languages' grammar. \textcite{woods_trans_1970} suggested Recursive Transition Networks (RTN) for sentence structure parsing, RTN essentially is a finite-state machine applied to syntax. 
\par \textcite{koskenniemi_twol_1983} created a model, which introduced an explicit formalism named Two-level morphology (TWOL) for describing morphological and morphonological paradigms. This model was capable of word-form recognition and production, but it was not yet compilable into finite-state machines, it was working at runtime and was known for being slow. Then \textcite{karttunen_twolc_1987} at Xerox Research Center developed a Two-level rule Compiler (\texttt{twolc}), which compiled TWOL rules into a finite-state machine. Later a separate compiler for lexicon definitions was introduced named \texttt{lexc} (\textbf{Lex}icon \textbf{C}ompiler) \parencite{karttunen_lexc_1993}, it came with its own formalism language for describing lexicon and morphotactics. The standard approach to modeling a language at that point was using \texttt{lexc} to describe lexicon and morphology and \texttt{twolc} to describe morphonology, which stayed almost the same to this day. 
\par HFST is a set of tools for creating and working with languages' morphology models in form of transducers \parencite{linden_hfst_2009}. It includes both \texttt{hfst-lexc} and \texttt{hfst-twolc} compilers, as well as command line interface commands for mathematical and other miscellaneous operations with transducers. One of the latest recent advances was the release of the \texttt{lexd} lexicon compiler \parencite{swanson_lexd_2021}. It is based on the \texttt{hfst-lexc} compiler but is claimed to be much faster in the compilation time.
\par FST (and HFST specifically) is widely applied when it comes to creating rule-based morphological models. Some of the latest examples of FST-based morphological tools are: morphological parser for the Tamil language by \textcite{sarveswaran_morph_2021}, a morphological transducer for Kyrgyz by \textcite{washington_finite_2012}, a morphological parser for Andi by \textcite{buntyakova_2023_twol} and a morphological parser for the Chamalal language by \textcite{budilova_2023_twol}.