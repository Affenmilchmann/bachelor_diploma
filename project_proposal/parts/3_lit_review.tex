\section{Literature review}
\todo{refine references; заменить дословные переводы названий работ чем-то получше}
\subsection{Shughni}
\par Shughni belongs to the Iranian family (Plungian 2022: 12). It is considered to belong to 'Pamiri' languages. According to Edelman and Dodykhudoeva (2009) estimates, approximately 80 000 - 100 000 people speak Shughni. There are two regions that speak Shughni: one on the territory of Afghanistan's Badakhshan Province - 'Shughnan', and the other on the territory of Tajikistan's Mountainous Badakhshan Autonomous Province - 'Shughnon' (Parker 2023: 2) (see Figure \ref{fig:map1}).
\par There are two main dictionaries of the Shughni language: 'Shughni texts and dictionary' (Zarubin 1960) and 'Shughni-Russian dictionary' (Karamshoev 1988-1999), both are written using Cyrillic script and include Russian translations. Some early dictionaries are 'Brief grammar and dictionary of Shughni' (Tumanovich 1906), that is also using Cyrillic and translates to Russian, and 'Shughni dictionary by D. L. Ivanov' (Salemann 1895), that translates to Russian but uses Arabic script alongside Cyrillic transcriptions for Shughhi word-forms.
\par Several Shughni grammar descriptions were written throughout the years, starting from basic grammar description done by D. L. Ivanov (Salemann 1895: 274-281). Latest significant works were 'Shughni language' (Эдельман Дж.И., Юсуфбеков Ш.П. 1999: 225-242), 'Сравнительная грамматика восточноиранских языков' (Edelman, Dodykhudoeva 2009) and 'A grammar of the Shughhi language' by (Parker 2023). The latter grammar by Parker will be the main theoretical source for the development of the morphology analysis tool since it is the biggest existing grammar, the most detailed and the most recent one.
\par A significant contribution to the Shughni NLP field is 'Digital Resources for the Shughni language' (Makarov et al., EURALI 2022). The authors, among other tools and resources, developed a rule-based morphological analysis tool for the Shughni language. The parser proposed in this work, while also being rule-based, differs in its implementation through the use of Helsinki Finite-State Technology (HFST).

\subsection{Morphology modeling}
\subsubsection{Neural approach}
\par One of the most recent and widely adopted approaches to morphology modeling involves the use of the Transformer-based deep learning models. This approach usually requires large amounts of training data in form of manually tagged word-forms, which is not available for Shughni. There are texts available to me, that were manually tagged at the Linguistic Convergence Laboratory of HSE university, which consist of 3453 tokens in total. While this amount of training data is small, it is worth mentioning that it is possible to train a Transformer-based model with such small datasets, as shown by Kondratyuk and Straka (2019). Their approach includes fine-tuning a pre-trained multilingual BERT model, authors conclude, that multilingual learning is most beneficial for low-resource languages, even ones that do not possess a training set.
\subsubsection{Rule-based approach}
\par Finite-state technology (FST) is a finite-state machine with two tapes, one for input strings and one for output strings. The machine maps the alphabet of the first string to the alphabet of the second string, this concept was first proposed by Mealy (1955) and Moore et al. (1956). Eventually linguists noticed this technology and started applying it to model natural languages' grammar. Woods (1970) suggested Recursive Transition Networks (RTN) for sentence structure parsing, RTN essentially is a finite-state machine applied to syntax. 
\par \todo{efficiency of fst}
\par FST is widely applied when it comes to creating rule-based tools. Some of the examples of FST-based morphological tools are: morphological parser for the Tamil language by Sarveswaran et al. (2021), a morphological transducer for Kyrgyz by Washington et al. (2012), a morphological analyzer and generator for Sakha by Ivanova et al. (2022) and a morphological analyser for the Laz language by Onal and Tyers (2019).

\par Linden et al. (2007)